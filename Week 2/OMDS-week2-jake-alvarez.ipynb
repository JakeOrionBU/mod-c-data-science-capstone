{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f167f66",
   "metadata": {},
   "source": [
    "### Section 1 Marketing Campaign \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358a532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Marketing] shape: (2216, 29)\n",
      "[Marketing] target distribution:\n",
      "Response\n",
      "0    1883\n",
      "1     333\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[Ridge] Best params: {'model__alpha': np.float64(316.22776601683796)}\n",
      "[Ridge] R2=0.3101 | MAE=0.2000 | RMSE=0.2968\n",
      "\n",
      "[Lasso] Best params: {'model__alpha': np.float64(0.01)}\n",
      "[Lasso] R2=0.2818 | MAE=0.2009 | RMSE=0.3028\n",
      "\n",
      "[ElasticNet] Best params: {'model__alpha': np.float64(0.01), 'model__l1_ratio': np.float64(0.7000000000000001)}\n",
      "[ElasticNet] R2=0.2964 | MAE=0.2002 | RMSE=0.2997\n",
      "\n",
      "Top features - Ridge\n",
      "                         feature  coefficient  abs_coeff\n",
      "5                   num__Recency    -0.060892   0.060892\n",
      "17             num__AcceptedCmp3     0.059695   0.059695\n",
      "19             num__AcceptedCmp5     0.055934   0.055934\n",
      "20             num__AcceptedCmp1     0.044966   0.044966\n",
      "8           num__MntMeatProducts     0.042899   0.042899\n",
      "15        num__NumStorePurchases    -0.040735   0.040735\n",
      "33   cat__Marital_Status_Married    -0.036436   0.036436\n",
      "34    cat__Marital_Status_Single     0.034743   0.034743\n",
      "35  cat__Marital_Status_Together    -0.034247   0.034247\n",
      "16        num__NumWebVisitsMonth     0.034212   0.034212\n",
      "\n",
      "Top features - Lasso\n",
      "                       feature  coefficient  abs_coeff\n",
      "17           num__AcceptedCmp3     0.063414   0.063414\n",
      "19           num__AcceptedCmp5     0.059842   0.059842\n",
      "5                 num__Recency    -0.059236   0.059236\n",
      "8         num__MntMeatProducts     0.045489   0.045489\n",
      "20           num__AcceptedCmp1     0.044160   0.044160\n",
      "16      num__NumWebVisitsMonth     0.031394   0.031394\n",
      "34  cat__Marital_Status_Single     0.024298   0.024298\n",
      "18           num__AcceptedCmp4     0.023116   0.023116\n",
      "15      num__NumStorePurchases    -0.021552   0.021552\n",
      "4                num__Teenhome    -0.020331   0.020331\n",
      "\n",
      "Top features - ElasticNet\n",
      "                         feature  coefficient  abs_coeff\n",
      "17             num__AcceptedCmp3     0.064032   0.064032\n",
      "5                   num__Recency    -0.062032   0.062032\n",
      "19             num__AcceptedCmp5     0.061000   0.061000\n",
      "8           num__MntMeatProducts     0.048930   0.048930\n",
      "20             num__AcceptedCmp1     0.045139   0.045139\n",
      "33   cat__Marital_Status_Married    -0.038300   0.038300\n",
      "16        num__NumWebVisitsMonth     0.033954   0.033954\n",
      "35  cat__Marital_Status_Together    -0.031650   0.031650\n",
      "15        num__NumStorePurchases    -0.029813   0.029813\n",
      "29            cat__Education_PhD     0.029799   0.029799\n"
     ]
    }
   ],
   "source": [
    "# --- Marketing Campaign Dataset ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# helpers for compatibility\n",
    "def rmse_score(y_true, y_pred):\n",
    "    try:\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "    except TypeError:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def make_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# load data \n",
    "df_marketing = pd.read_csv(\"marketing_campaign_cleaned.csv\")\n",
    "target = \"Response\"\n",
    "\n",
    "print(\"[Marketing] shape:\", df_marketing.shape)\n",
    "print(\"[Marketing] target distribution:\")\n",
    "print(df_marketing[target].value_counts())\n",
    "\n",
    "X = df_marketing.drop(columns=[target])\n",
    "y = df_marketing[target]\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# preprocessing \n",
    "numeric_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", make_ohe())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_pipe, numeric_cols),\n",
    "    (\"cat\", categorical_pipe, categorical_cols)\n",
    "])\n",
    "\n",
    "# CV + grids \n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "ridge_grid = {\"model__alpha\": np.logspace(-3, 3, 13)}\n",
    "lasso_grid = {\"model__alpha\": np.logspace(-3, 1, 9)}\n",
    "enet_grid  = {\"model__alpha\": np.logspace(-3, 1, 9),\n",
    "              \"model__l1_ratio\": np.linspace(0.1, 0.9, 9)}\n",
    "\n",
    "# run model helper \n",
    "def run_model(name, model, grid):\n",
    "    pipe = Pipeline([(\"pre\", preprocessor), (\"model\", model)])\n",
    "    search = GridSearchCV(pipe, grid, cv=cv,\n",
    "                          scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    search.fit(X, y)\n",
    "    y_pred = search.best_estimator_.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = rmse_score(y, y_pred)\n",
    "    print(f\"\\n[{name}] Best params: {search.best_params_}\")\n",
    "    print(f\"[{name}] R2={r2:.4f} | MAE={mae:.4f} | RMSE={rmse:.4f}\")\n",
    "    return search.best_estimator_\n",
    "\n",
    "# run all three \n",
    "ridge_best = run_model(\"Ridge\", Ridge(), ridge_grid)\n",
    "lasso_best = run_model(\"Lasso\", Lasso(max_iter=20000, random_state=42), lasso_grid)\n",
    "enet_best  = run_model(\"ElasticNet\", ElasticNet(max_iter=20000, random_state=42), enet_grid)\n",
    "\n",
    "# Feature Importance (Top 10)\n",
    "def show_top_features(estimator, X):\n",
    "    pre = estimator.named_steps[\"pre\"]\n",
    "    model = estimator.named_steps[\"model\"]\n",
    "    try:\n",
    "        feature_names = pre.get_feature_names_out()\n",
    "    except:\n",
    "        feature_names = list(range(len(model.coef_)))\n",
    "    coefs = model.coef_\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"coefficient\": coefs,\n",
    "        \"abs_coeff\": np.abs(coefs)\n",
    "    }).sort_values(\"abs_coeff\", ascending=False).head(10)\n",
    "    print(coef_df)\n",
    "\n",
    "print(\"\\nTop features - Ridge\")\n",
    "show_top_features(ridge_best, X)\n",
    "\n",
    "print(\"\\nTop features - Lasso\")\n",
    "show_top_features(lasso_best, X)\n",
    "\n",
    "print(\"\\nTop features - ElasticNet\")\n",
    "show_top_features(enet_best, X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39134b",
   "metadata": {},
   "source": [
    "### Observations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3e3c21",
   "metadata": {},
   "source": [
    "## Marketing Campaign – Observations\n",
    "\n",
    "### Model Performance\n",
    "- **Ridge (α ≈ 316.2)** performed best with an **R² of 0.3101**, slightly ahead of Elastic Net (0.2964) and Lasso (0.2818).  \n",
    "- Across all models, **MAE ≈ 0.20** and **RMSE ≈ 0.30**, indicating relatively small errors but also showing the models struggle to capture more than ~31% of the variance.  \n",
    "- This aligns with your earlier EDA and Random Forest analysis: **marketing response is difficult to predict linearly** due to diffuse relationships and class imbalance (responders vs. non-responders).  \n",
    "\n",
    "---\n",
    "\n",
    "### Feature Importance\n",
    "**Consistently strong predictors across all three methods:**\n",
    "- **Campaign acceptance indicators**: `AcceptedCmp3`, `AcceptedCmp5`, `AcceptedCmp1`  \n",
    "  (positive influence → past acceptance increases response likelihood).  \n",
    "- **Recency** (negative coefficient → more recent customers are more likely to respond).  \n",
    "- **MntMeatProducts** (positive → higher spending on meat correlates with responsiveness).  \n",
    "- **NumWebVisitsMonth** (positive → digital engagement correlates with campaign response).  \n",
    "\n",
    "**Demographics (Marital_Status, Education)** showed up in Ridge and Elastic Net but not as strongly in Lasso, suggesting these features are secondary drivers once regularization penalizes weaker signals.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "- The results confirm that **past campaign interactions and spending patterns** are the most valuable predictors for marketing responsiveness.  \n",
    "- **Demographics alone** (e.g., marital status, education) play a smaller role compared to behavioral indicators.  \n",
    "- The relatively modest R² values highlight the **inherent difficulty of predicting rare events (response = 1)** with linear methods — the imbalance (333 responders vs. 1883 non-responders) limits accuracy.  \n",
    "- **Elastic Net’s blend of Ridge and Lasso** helped balance between feature selection and coefficient shrinkage, surfacing both behavioral and demographic features.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76391cb",
   "metadata": {},
   "source": [
    "### Customer Churn Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e80fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Churn] shape: (440832, 12)\n",
      "[Churn] target distribution:\n",
      "Churn\n",
      "1.0    249999\n",
      "0.0    190833\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[Ridge] Best params: {'model__alpha': np.float64(0.001)}\n",
      "[Ridge] R2=0.7854 | MAE=0.1829 | RMSE=0.2295\n",
      "\n",
      "[Lasso] Best params: {'model__alpha': np.float64(0.001)}\n",
      "[Lasso] R2=0.7853 | MAE=0.1831 | RMSE=0.2296\n",
      "\n",
      "[ElasticNet] Best params: {'model__alpha': np.float64(0.001), 'model__l1_ratio': np.float64(0.1)}\n",
      "[ElasticNet] R2=0.7854 | MAE=0.1830 | RMSE=0.2295\n",
      "\n",
      "Top features - Ridge\n",
      "                           feature  coefficient  abs_coeff\n",
      "0                  num__CustomerID    -0.302301   0.302301\n",
      "14    cat__Contract Length_Monthly     0.105317   0.105317\n",
      "4               num__Support Calls     0.095997   0.095997\n",
      "6                 num__Total Spend    -0.062226   0.062226\n",
      "13     cat__Contract Length_Annual    -0.052772   0.052772\n",
      "15  cat__Contract Length_Quarterly    -0.052547   0.052547\n",
      "5               num__Payment Delay     0.040715   0.040715\n",
      "8               cat__Gender_Female     0.025847   0.025847\n",
      "9                 cat__Gender_Male    -0.025847   0.025847\n",
      "1                         num__Age     0.023147   0.023147\n",
      "\n",
      "Top features - Lasso\n",
      "                         feature  coefficient  abs_coeff\n",
      "0                num__CustomerID    -0.303325   0.303325\n",
      "14  cat__Contract Length_Monthly     0.152134   0.152134\n",
      "4             num__Support Calls     0.095703   0.095703\n",
      "6               num__Total Spend    -0.061708   0.061708\n",
      "8             cat__Gender_Female     0.047590   0.047590\n",
      "5             num__Payment Delay     0.040078   0.040078\n",
      "1                       num__Age     0.022417   0.022417\n",
      "7          num__Last Interaction     0.021433   0.021433\n",
      "2                    num__Tenure    -0.005173   0.005173\n",
      "3           num__Usage Frequency    -0.004951   0.004951\n",
      "\n",
      "Top features - ElasticNet\n",
      "                         feature  coefficient  abs_coeff\n",
      "0                num__CustomerID    -0.302157   0.302157\n",
      "14  cat__Contract Length_Monthly     0.138684   0.138684\n",
      "4             num__Support Calls     0.096034   0.096034\n",
      "6               num__Total Spend    -0.062227   0.062227\n",
      "5             num__Payment Delay     0.040692   0.040692\n",
      "8             cat__Gender_Female     0.029722   0.029722\n",
      "1                       num__Age     0.023102   0.023102\n",
      "7          num__Last Interaction     0.022487   0.022487\n",
      "9               cat__Gender_Male    -0.021571   0.021571\n",
      "13   cat__Contract Length_Annual    -0.018171   0.018171\n"
     ]
    }
   ],
   "source": [
    "# Customer Churn Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# helpers for compatibility\n",
    "def rmse_score(y_true, y_pred):\n",
    "    try:\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "    except TypeError:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def make_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# load data\n",
    "df_churn = pd.read_csv(\"customer_churn_cleaned.csv\")\n",
    "target = \"Churn\"\n",
    "\n",
    "print(\"[Churn] shape:\", df_churn.shape)\n",
    "print(\"[Churn] target distribution:\")\n",
    "print(df_churn[target].value_counts())\n",
    "\n",
    "X = df_churn.drop(columns=[target])\n",
    "y = df_churn[target]\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# preprocessing \n",
    "numeric_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", make_ohe())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_pipe, numeric_cols),\n",
    "    (\"cat\", categorical_pipe, categorical_cols)\n",
    "])\n",
    "\n",
    "# CV + grids \n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "ridge_grid = {\"model__alpha\": np.logspace(-3, 3, 13)}\n",
    "lasso_grid = {\"model__alpha\": np.logspace(-3, 1, 9)}\n",
    "enet_grid  = {\"model__alpha\": np.logspace(-3, 1, 9),\n",
    "              \"model__l1_ratio\": np.linspace(0.1, 0.9, 9)}\n",
    "\n",
    "# run model helper \n",
    "def run_model(name, model, grid):\n",
    "    pipe = Pipeline([(\"pre\", preprocessor), (\"model\", model)])\n",
    "    search = GridSearchCV(pipe, grid, cv=cv,\n",
    "                          scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    search.fit(X, y)\n",
    "    y_pred = search.best_estimator_.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = rmse_score(y, y_pred)\n",
    "    print(f\"\\n[{name}] Best params: {search.best_params_}\")\n",
    "    print(f\"[{name}] R2={r2:.4f} | MAE={mae:.4f} | RMSE={rmse:.4f}\")\n",
    "    return search.best_estimator_\n",
    "\n",
    "# run all three \n",
    "ridge_best = run_model(\"Ridge\", Ridge(), ridge_grid)\n",
    "lasso_best = run_model(\"Lasso\", Lasso(max_iter=20000, random_state=42), lasso_grid)\n",
    "enet_best  = run_model(\"ElasticNet\", ElasticNet(max_iter=20000, random_state=42), enet_grid)\n",
    "\n",
    "# Feature Importance (Top 10) \n",
    "def show_top_features(estimator, X):\n",
    "    pre = estimator.named_steps[\"pre\"]\n",
    "    model = estimator.named_steps[\"model\"]\n",
    "    try:\n",
    "        feature_names = pre.get_feature_names_out()\n",
    "    except:\n",
    "        feature_names = list(range(len(model.coef_)))\n",
    "    coefs = model.coef_\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"coefficient\": coefs,\n",
    "        \"abs_coeff\": np.abs(coefs)\n",
    "    }).sort_values(\"abs_coeff\", ascending=False).head(10)\n",
    "    print(coef_df)\n",
    "\n",
    "print(\"\\nTop features - Ridge\")\n",
    "show_top_features(ridge_best, X)\n",
    "\n",
    "print(\"\\nTop features - Lasso\")\n",
    "show_top_features(lasso_best, X)\n",
    "\n",
    "print(\"\\nTop features - ElasticNet\")\n",
    "show_top_features(enet_best, X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896d360",
   "metadata": {},
   "source": [
    "## Customer Churn – Observations\n",
    "\n",
    "### Model Performance\n",
    "All three models performed very similarly:\n",
    "\n",
    "- **Ridge (α ≈ 0.001)**: R² = 0.7854, MAE = 0.1829, RMSE = 0.2295  \n",
    "- **Lasso (α ≈ 0.001)**: R² = 0.7853, MAE = 0.1831, RMSE = 0.2296  \n",
    "- **Elastic Net (α ≈ 0.001, l1_ratio = 0.1)**: R² = 0.7854, MAE = 0.1830, RMSE = 0.2295  \n",
    "\n",
    "The R² of ~0.79 indicates the models explain nearly 80% of the variance in churn outcomes — far higher than what we saw in the Marketing dataset.  \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Importance\n",
    "**Dominant predictors:**\n",
    "- **Contract Length_Monthly** → large positive coefficient: customers with monthly contracts are more likely to churn.  \n",
    "- **Support Calls** → positive influence: frequent support calls correlate with higher churn risk.  \n",
    "- **Payment Delay** → positive: delays in payments increase churn probability.  \n",
    "- **Total Spend** → negative: higher spending customers are less likely to churn.  \n",
    "\n",
    "**Moderate predictors:**\n",
    "- **Gender**: Female positive, Male negative (but relatively smaller coefficients).  \n",
    "- **Age** and **Last Interaction** had small but consistent positive effects.  \n",
    "- **Tenure** and **Usage Frequency** showed weak negative relationships (slight retention effect).  \n",
    "\n",
    "**CustomerID** appears with a large coefficient in all models , this is likely an artifact of ID encoding and should be excluded from modeling since it carries no business meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "The results strongly reinforce the story from your EDA and Random Forest:\n",
    "\n",
    "- **Contract structure, service interaction, and payment behavior** are the most powerful churn drivers.  \n",
    "- Customers on flexible (monthly) contracts with repeated support issues and late payments are the **highest churn risk**.  \n",
    "- Stable, higher-spending customers are **least likely to leave**.  \n",
    "- Linear models here perform well because the **relationships are direct and structured** — unlike the marketing responsiveness case, churn behavior shows clearer patterns.  \n",
    "- Removing `CustomerID` in future iterations will refine interpretability further and avoid misleading importance rankings.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
